<!DOCTYPE html>
<html>
<head>
	<meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" href="/assets/css/proj.css">
	<title>Deep Learning for Garbage Segregation</title>
</head>
<body class="user-details">
	<h1 align="center">Deep Learning for Garbage Segregation</h1>
	<p>We produce unbelievable amounts of garbage and still do not have proper measures in place to deal with it. The amount of waste generated has tripled since 1960, leading to devastating effects for the environment. Segregating waste for further processing is the first major bottleneck in the recycling process. This task is usually carried out by hand. Automating it would greatly help reduce the cost and time taken to recycle items. <br>

This project aims to solve the fundamental problem of waste segregation using <b>Deep Learning</b>. I want to develop Computer Vision technologies that can classify the category that individual garbage pieces belong to. This can then be implemented in smart waste disposal systems, either in the form of smart bin that immediately sorts garbage into the category it belongs, or as a additional module in existing garbage processing facilities.</p>
<h2 align="center">Work Done So Far</h2>
<h3>Literature Review</h3>
<p> Looked into existing smart waste disposal systems and identified potential improvements that could be made. Shortlisted various Image processing and Deep Learning techniques that may help us.</p>
<h3>Initial Dataset</h3>
<p>
 The dataset was obtained from the <a href="https://github.com/garythung/trashnet">trashnet Github repository</a>. It contains images split into 6 different classes: glass, paper, cardboard, plastic, metal, and other trash . Currently, the dataset consists of 2527 images: 
 <ul><li>
 	501 glass 
 </li>
<li>
	594 paper
</li>
<li>
	482 plastic
</li>
<li>
	410 metal 
</li>
<li>
	137 trash
</li></ul>
 The pictures were taken by placing the object on a white poster-board and using sunlight and/or room lighting . The pictures have been resized down to 512 x 384. The devices used were Apple iPhone 7 Plus, Apple iPhone 5S, and Apple iPhone SE. </p>
<h3>
	Model v1
</h3>
<p>
	I attempted training our dataset on existing Neural architectures (such as VGG19). However due to computational constraints this approach was soon abandoned. I tried training the data on simple CNNs built from scratch but the accuracy was too low.
</p>
<h3>Model v2</h3>
<p>
I decided to use Transfer Learning as the primary approach for designing our model. Used Resnet50 pretrained on Imagenet as a feature extractor and used Pytorch to retrain the final layer on our dataset. 

</p>
<img src="/assets/img/resnetcm.png",alt="resnet confusion matrix", align="center">
<p>We can see in the Confusion Matrix that the performance of each category is almost equally good. ’Glass’ and ’Trash’ perform slightly lower than other categories. Accuracy is 95.25% which is quite good.</p>
<h3>Model Demo Frontend</h3>
<p>Deployed our model to a simple website (https://recycle.onrender.com) that can be used for demonstration and testing. 
<!---
pic of ui
-->
</p>
<h2>Designing a Smart Bin Prototype</h2>
<p>The high level idea was that I wanted the user experience to be identical to if they were using a regular bin.</p>
<h3>
	Proposed Working of a Smart Bin
</h3>
<ol>
	<li>The user drops the garbage object into the smart bin.</li>
	<li>It lands on a staging area where a camera is focussed.</li>
	<li>A motion detection script running on the microcontroller triggers the image classification algorithm.</li>
	<li>The image classification algorithm runs locally and assigns a label to the object.</li>
	<li>The microcontroller then instructs the servo motors on the correct alignment. It then uploads the image and the corresponding label to the database in the cloud.</li>
	<li>The staging area drops and the servo motors guide the object to the area that corresponds to the assigned label. </li>
</ol>
	<p>Periodically infrared sensors check whether the dustbin is at capacity. If so, an alert is sent to the backend. This can be used for route planning.</p>
<h3>
	Hardware Used for Prototype
</h3>
<ul>
	<li>Raspberry Pi 3B</li>
	<li>720p Camera Module</li>
	<li>Infrared sensors</li>
	<li>Servo motors</li>
	<li>Connecting Cables</li>
	<li>Battery Pack</li>
</ul>


<h3>Model v3</h3>
<p>Running our model on the Raspberry Pi to classify images in real-time was impractical. The Pi would heat up and would take too long to predict the class of an image. Due to computational constraints I had to sacrifice accuracy for speed and created a new model by performing Transfer Learning on MobileNet v2. I then exported this to a Ternsorflow Lite model. This resulted in our model classifying our images almost instantaneously, although accuracy was affected.</p>
<h3>Model v4</h3>
<p>I was able to improve model performance on the pi by performing finetuning over NASNetMobile pretrained on imagenet. The dataset was ﬁrst split in ratio of 0.8-0.1-0.1 (train-validation-test). </p>
<img src="/assets/img/traindataset.png",alt="dataset", align="center">
<p>Weights until layer 749 were frozen and the rest were allowed to train. Then I added 3 fully connected layers with a dropout of 0.3. The ﬁrst 2 layers used ReLU activation function and the last layer used Softmax. The optimizer used was ADAM. Early Stopping and ReduceLRonPlateau were used as callbacks to prevent over-ﬁtting and minimize loss respectively. </p>
<img src="/assets/img/nasnetcm.png",alt="nasnet confusion matrix", align="center">
<p>We can see that the performance of ’Paper’ and ’Trash’ is slightly lower than the other 4 categories. The model is comparatively not as good at distinguishing between ’Paper’ and ’Trash’, with 17 ’Trash’ objects being classiﬁed incorrectly as ’Paper’.</p>
<img src="/assets/img/nasnetcr.png",alt="nasnet classification report", align="center">
<p>In this use case precision and recall are arguably equally important so f1 score is a good metric to judge model performance. ’Cardboard’ has the highest F1 score of ’0.86’. ’Trash’ has a low score, probably because of the lesser number of ’Trash’ images in the data-set compared to the other categories and due to variety of images under the category.</p>
<h3>Dataset Augmentation</h3>
<p>
After every classification the image, coresponding label and a timestamp is sent to a Firebase database. This allows us to retrain our model on images taken by our camera in our lighting and background conditions, which should significantly improve our accuracy.</p>
<img src="/assets/img/firebase1.png",alt="firebase realtime database", align="center">
<h3>Conclusion</h3>
<p>Waste collection and disposal will be at the forefront for technological revolutions in the coming years. This project on waste segregation tackles only a very small portion out of the large number of processes that are involved in the efficient disposal of waste. Cutting edge technology including AI and IoT need to be used in order to containe this rapidly growing problem.</p>
</body>
</html>